# StackOverflow Tag Prediction wiht Apache Spark

#### Project description

The aim of the project is to develop a tool that predict tags for the StackOverflow questions, and also predict if a given question will receive an answer or not. Another objective is to verify the performance obtained by using Apache Spark with the university cluster, and the performance obtained by using it on a single machine. The project is developer in python language and ApacheSpark engine. The result should verify that there is an increase in computation speed when we use parallel computation in the context of cloud computing. For the machine learning part of the project I use the `pyspark.ml` module which is a DataFrame-based module that allowed me to quickly assemble and configure the required machine learning tools.

#### Data Retrieval

The first step was to retrieve the required data from StackOverflow. After some research I found the data that I needed on Kaggle. It consist of three tables, one for the questions, one for the answers and one for the tags. The tables have 10% of the questions and answers on StackOverflow, and they were selected by picking one question every 10. The tables can be downloaded in a csv format directly from the Kaggle platform. The data spans a time interval from 2006 to 2016 and it contains:

-   1.264.216 questions
-   2.014.516 answers
-   3.750.994 tags

The tables have the following columns:

-   Questions: `id`, `title`, `body`, `creationDate`, `closedDate`, `score`, and `ownerUserId`
-   Answers: `id`, `body`, `creationDate`, `score`, `ownerUserId` and `parentId` (which links the answer with the question
-   Tags: `id`, `Tag`(in this case the id of the tag associate it with the question).

#### Data Preparation

I first created aSpark Sessionvariable which will be used to load and work with the data in the files. I loaded the files into dataframes using `pyspark.sql` module. To be able to apply the analysis that I had in mind, I had to do some manipulations of the tables, and to create derived ones. The first thing was to define a schema for each table. I had to do this because the inferred schema was wrong. So I defined a `StructType` for each database with the name of the fields and the types. Next for each table I used the spark session variable to load the data from the csv files to dataframes. I used the `multiline` option which allowed me to read multiple lines at the same time form the csv file. This is needed because in the original csv file there are rows that span on multiple lines. Then i passed the `escape` option with the `\"` value. This will escape all the "(quotation marks). The last option was `header` to `true` which inform spark that the first line of the file contains the headers. Loading the data into dataframe format facilitates allot the manipulation and the interrogation of the data.

#### Data Analysis

Since the project is based on a Machine Learning approach, I did some data analysis in order to better understand the data that I am working with. This is very necessary because in order to obtain good result in applying ML techniques, it is important to know the structure of the data.

##### Questions by year

_Logic_. In this case I wanted to know what is the distribution of the questions over the years. This is important to spot trends and outliers. For example if I want to use only the data for a particular year or a range of years, I need to know if that data is a good representative of the entire dataset. What I found out is that there was a continuous increase in the number of questions from 2006 to 2015. In Figure 1 it can be seen the plot representing this trend.
_Implementation_. From an implementation point of view what I did was to use the `groupby` method on the `CreationDate` column of the questions table, and then used the `count` method which returned me how many questions are per each year.

##### Questions by month

_Logic_. This analysis counts the number of questions for each month independently of the year. The purpose of the analysis in the same as in the previous section. As expected the number of questions is lower in the holiday months (November and December), but also in January and February which I could not explain why. The plot with the results is presented in Figure 2._Implementation_. Here I used the same approach in the implementation as in the previous section, the difference is that I used the month instead of the year.

##### Answered questions per month

_Logic_. Here I retrieve the number of answered questions for each month. This is important because tell me which are the months in which is more likely to get an answer to a question. The distribution, as it can be seen from Figure 3 is pretty much the same as the in the above section analysis.
_Implementation_. In this case I had to first join the `Questions` and the `Answers` tables and select only the `Question.Id` and the `question.CreationDate`. Then I used `dropDuplicates` method to drop the duplicated rows, and finally I applied the same procedure as in the section above to get the number of answered questions per month.

##### Top 20 tags

_Logic_. In this analysis I want to find out what are the 20 most used tags on StackOverflow questions. I wanted to understand if there are tags that have a big impact in the prediction. In other words I want to see if there are tags that are allot more frequent than others, because if this is the case then the prediction will be biased. As it can be seen from Figure 4 there is a quadratic decrease in the frequency of the tags, so I decided that there is no need for extra data manipulation for the tags data. If the distribution was a lot unbalanced, then an extra data manipulation step would be required to balance it
_Implementation_. Here I had to join the `Questions` and the `Tagstables` and apply the `groupby` and `count` methods to obtain the desired result.

##### Top 20 tags in answered questions

_Logic_. This analysis tell me which are the tags that I can use for a question in order to increase the probability of receiving an answer. As it can be seen from Figure 5 the plot is almost identical to the results in the above section section. This is normal because obviously higher is the number of questions in which a tag is present, higher is the probability to be in answered questions. For this reason this analysis is not very accurate, or better it is not quite true that if we use for example the javascript tag, this will absolutely increase the probability for the question to be answered. The reason is the one explained above, which is, javascript has the highest number of answered questions because it has the highest number of questions in general.
_Implementation_. I first created the `AnsweredQuestions` table by joining the `QuestionsandAnswers` tables, and then used the `groupby` method and `count` on the `Tag` field.

##### Top 20 tags in not answered questions

_Logic_. In this analysis I wanted to see what is the distribution of the tags in the not answered questions. As it can be seen from Figure 6 it is quite different from the above result. This analysis tell me which are the tags that are most likely to not receive a response.
_Implementation_. Here I used the same logic as in the above section.

##### Top 20 tags by questions up votes

_Logic_. Here I analyze the distribution of the number of tags by questions up votes. In other words I took the tags that are present in questions that have the highest sum of up votes. In this case the upvote is not directly related with the tag but with how the question is posed, and how interesting is. The results can be seen in Figure 7.
_Implementation_. In this case I first join the `Questions` and the `Tags` tables, then I do a `groupby` on the `Tags` field, and finally I use the `agg` and `sum` functions to find the result.

##### Number of tags per answered questions

_Logic_. In this analysis I try to understand what is the most suitable number of tags to insert in a question. To do this I see how many answered questions have 1 tag, 2 tags, up to five tags. As it can be seen from Figure 8, most of the answered questions have 2 or 3 tags. This analysis allow me to understand how many tags should I predict for a given question.
_Implementation_. To obtain this result I joined the `AnsweredQuestions` and the `Tags` DataFrames, then I used the `groupby` method on the `AnsweredQuestions.Id`. Next I used the `agg` and `count` methods on the above result to obtain the number of tags for each answer. Finally I used another `groupby` and a `count` method on the last result to obtain the final result.

##### Number of tags per unanswered questions

_Logic_. Surprisingly the distribution in this case is quite the same as in the section above. I would expected that for non answered questions to have a different distribution (higher number of question swith one tag for example). This could mean that using these statistics for choosing the number of tags per question could not be the optimal choice, but since the main purpose of the project is to test Apache Spark performance on the cluster, I decided to use it nonetheless. In Figure 9 it is presented the results of this analysis.
_Implementation_. In this case the procedure is the same as in the above section.

##### Number of tags per up voted questions

_Logic_. Taking into account the considerations in the section above, I did this analysis so that I could have more information about the distribution of the number of tags per question. As it can be seen from Figure 10 the mean (red line) is about 3 tags per question, which enforces my final choice to use this value for the number of tags to predict for a given question.
_Implementation_. For implementing this logic I first joined the `Questions` and the `Tags` tables, then used the `groupby` method on both `Questions.Id` and `Questions.Score` columns. On the above result I then used the `agg` method and the `count` method to obtain the final result. The execution time in the analysis part is computed computed considering the loading of the data into DataFrames, and all the above analysis steps. So is the sum of the execution time of all the above sections plus the time for loading the data. I run multiple executions of the analysis to see if I will have a significant difference in execution time over different executions, but the variation was minimal.

#### Tag Prediction

_Logic_. To complete this task I’m going to use tools both from Natural Language Processing and Machine Learning domains. The logic is to use Logistic Regression and build a Multiclass Classifier which given the text of a question it associate a probability with each tag present in the collection of tags. The tags with the highest probability, are the tags that are more likely to be associated with the given question. So given that I decided to use three tags per question, the tags are represented by the first three tags with the highest probability. In order to be able to use logistic regression on the data I first have to transform the text in a format which can be processed. So what I have to do is to represent the text as vectors, and here is where the NLP is used. In particular I use Inverse Document Frequency (IDF) to associate to each word a numeric value. In order to obtain a good result with IDF, I have to first preprocess the text using other NLP tools like `stemming`, `punctuation removal`, `stopwords removal`, `html tags removal`, etc.
_Implementation_. Here I’m going to describe the steps that I did to obtain the tags prediction. First I had to join the `Questions` and the `Tags` tables, and use the filter method in order to keep only the questions that had a score above some given threshold. I did this because there were too many questions, and because I consider that the questions that have a higher score are the ones that are more reliable for the tags prediction. From the obtained table I retrieved the tags with the highest frequency. In order to obtain the mentioned table I first used the `groupby` method on the `Tag` field and then used the `count` method to count how many tags there are for each question. What I did next was to use the `limit` method in order to keep only the rows that had at least `n` tags for question, where `n` is the tags frequency threshold decided arbitrarily. I did this for the same reason as above, there were too many tags to compute for the purpose of this project. Now I retrieve all the rows from the `QuestionsTags` table that have relevant tags. By relevant tags I mean elements that are also in the second retrieved table, which is the `HighestFrequencyTags`. On the obtained table I apply for the `Title` column the following NLP tools:

-   remove the punctuation
-   remove the stopwords
-   stemming the words
-   make all the text lowercase

For the `Body` column instead I apply the following operations:

-   remove html tags
-   remove punctuation
-   remove stopwords
-   stem the words
-   make all the text lowercase

Next step is to create from the obtained table the features that will be used in the Machine Learning part. To do this I first tokenize the text in the `Body` column. Then I map the obtained terms (tokens) to the term frequency by using the HashingTF. In other words it converts the terms in fixed-length feature vectors. I now use IDF on those feature vectors to obtain the IDF for each term. In other words this scales each feature, or more precisely it down-weights features which appear frequently in a corpus. Now that I have the data features, I create a training and a test set by using the `randomSplit` method provided by `pyspark.ml` module. Then I create an Logistic Regression model and call the `fit` method on the training data. This will adjust the model weights based on the training data. At this point I can call the `transform` method on the test data, and create a MulticlassClassificationEvaluator in order to test the accuracy of the model.

For the tag prediction, the execution time is computed considering the loading of the data into DataFrames, and all the operations that I did to process and modify the data. These operations consist in joining the tables, grouping, filtering, etc. The I had the NLP operations which consist in stemming, tags removal, punctuation removal, etc. The final computation part that contributed to the execution time was the machine learning part in which I created the model did the training and the prediction.

#### Answers Prediction

_Logic_. To implement this step I used the exact same logic as in the section5, but obviously using different tables. One fact that I had to consider in this case is data unbalancing. The number of answered questions is much higher then the number on not answered questions, so whatI did was to consider the same number of answered questions as the number of not answered questions.
_Implementation_. To do this I ordered the questions by score and took only the first of them, where is the number of not answered questions. In this case as in the previous section, the execution time is computed over all the operations, such as loading the data, processing the data, applying the NLP transformations, and creating and training the prediction model.

#### Considerations

I encountered some major problems in running the application on the provided cluster, in particular running the application on multiple nodes. The major problem was that the library that I used in the project (nltk) was not installed on all the nodes, so as soon as the execution started on anode different from the one that I launched the program, an error was triggered saying that nltk is not present. I run the application on both university clusters (the old one and the new one) but I encountered the problem on both. Since I wasn’t able to fix the problem by myself, I contactedFederico Dassereto. After some analysis we understood that there was a problem with the cluster. In particular the spark execution didn’t propagated the dependencies to all the nodes. One strange thing was that it happened only with NLP libraries. I tried different ones likeSpaCy, gensim, SnowbalStemmer, PorterStemmer, etc. For example didn’t happened with libraries like numpy, pandas, etc. Since we were not able to fix this issue, what I did was to download the source code of the nltk library and use the option `--py-files` to "inject" the nltk classes that I needed on all the nodes, and this solved the problem. I encountered some other strange behaviors for what concern the Spark execution. The first thing is that from the INFO provided during the execution, I noticed that Spark used only at maximum two nodes and two executor (usually only one node and one executor). I first mention that I obviously launched the execution with `--master yarn` option, which should take care of the data distribution and partitioning. Indeed from the INFO I could see that the partitions were 200 but the nodes and the executors only one. After allot of research, try and failures, and after excluding the possibility that this was a data related or implementation related problem (I analysed all the execution steps and the data produced), I found out that the problem was that the data was present only on one node, so for the data locality principle applied by Spark it was using only the node on which the data was present. At this point I controlled if the data was correctly copied on the hdfs system, and found out that indeed the data was on the hdfs system and that the program was reading it from there. I also repartitioned the data from python, even if as already mentioned the INFO showed me that the data is partitioned. Since everything seemed fine I searched for a solution to how to avoid data locality. WhatI did, was to pass the `--conf "spark.locality.wait.node=0"` configuration, which ignore the data locality principle. I also passed the `--num-executors`, and with this two configurations I finally manage to run the program on multiple nodes. In fact the execution time doped from 500 seconds more or less to 100 second. At this point I consulted with Federico to try and find why the data is not replicated on all the nodes. After some further analysis we discovered that the replication factor was 1. So Federico suggested me to run the `hadoop fs -setrep -w <repnum> <path>` which would set the number of replications to `repnum`. Unfortunately this not solved the problem,since if I launch the program without the data locality configuration, it still uses only one node. I tried to increase the number of replications but still no success. I arrived at the conclusion that perhaps the Spark configuration could be the problem. At this point given also that I was working on this project for quite some time now, I decided to use the aforementioned configuration for ignoring the data locality, since allowed me to obtain a quite good. The last major problem that I encountered was related again to the nltk library. As already mentioned I downloaded the source code of the modules that I needed and shipped them with the execution. The problem is that in the case of the answers prediction, I obtain an error of `"Recursion Error: Maximum Recursion depth exceeded"` on the Questions.Body field. After controlling that after the processing the data in that column was ok, I tried to figure out where the problem could be. What I noticed is that happened inside the nltk stem method, and not in my code. The first attempt to fix this problem was to increase the recursion limit in python. Even if I increased the recursion limit by quite allot, the problem persisted. After some other research I found out that this problem could be triggered by some particular text which is not a valid text to be processed by the stem method. An example could be text that does not represent actual words, of for example text that finishes with allot of consonants. What I understood is that nltk uses recursion to compute where to chop of the word in the stemming process and if the word has a particular format, this process of recursion could end in an infinite recursion loop, and hence produce the experienced error. This is very likely for what concern the text on the StakOverflow because as already mentioned could possibly contain a string of 100 "y" letters, if the user had to use this string as example, or maybe some very long URL, etc. Unfortunately I wasn’t able to fix this problem on the cloud so what I did was to only simulate the stemming by splitting and recombining the considered string of text. One curious thing is that in local and on the tags prediction this does not happened, but in local I’m using the nltk modules from the pip installation. The difference between the tags prediction and the questions prediction is that in the questions prediction the text length that I pass to the stem method is much larger than in the case of tag prediction. The explanation that I give to myself is that the code that I used on the cloud is not the updated one and this is what could cause the problem, because in local the answers prediction stemming worked just fine. Unfortunately these is the only nltk code that I found available.

#### Results

As already mentioned the objective of this project was to test Apache Spark performance on the cloud provided by the university. I first installed and configured Spark locally on my computer so that I can confront the execution times between the local and cloud execution. I could also run the program using `--master local` which would force the execution on only one node of the cluster. I confronted four different execution times: the analysis, the tag prediction, the answers predictions and all the three executions together. In order to obtain a better result I used the Cross Validation technique, but since it took quite a wile to run, what I did was to run it only on the cloud, find the best tuning parameters, and then make the execution confront using the best parameters. In Table 1 are presented the results that I obtained expressed in seconds. As it can be noted, there is an significant increase in execution speed on the cloud with respect the local PC execution. The Table 2 shows the accuracy that I obtained for the tag prediction and the answers prediction, both on local PC and cloud. Considering the nature of the problem (multiclassclassification) I think that the precision is quite high for the tag prediction. For what concern the answers prediction I think that the accuracy is so high because the problem is a binary classification problem, and because the two sets are very well distinguishable (answered and not answered questions).
